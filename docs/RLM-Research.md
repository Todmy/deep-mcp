# Recursive Language Models: the emerging paradigm reshaping LLM inference

**Recursive Language Models (RLMs) represent a newly formalized inference paradigm — introduced by MIT researchers in late 2025 — in which an LLM stores its input prompt as an external variable and recursively invokes itself (or sub-models) on decomposed snippets of that context, rather than processing the entire input in a single forward pass.** This decouples the model's reasoning from its context window, enabling processing of inputs two orders of magnitude beyond standard limits (10M+ tokens tested). The concept crystallizes years of prior work on recursive prompting, iterative self-refinement, and looped transformer architectures into a specific, implementable framework. RLMs are distinct from both traditional autoregressive LLMs (which consume all context in one pass) and classic Recursive Neural Networks (which apply shared weights over tree-structured inputs at the architecture level). As of early 2026, RLMs are rapidly gaining traction — already integrated into the DSPy framework and adopted by Prime Intellect as a core research direction — but have not yet reached production deployment.

## From blog post to paradigm in three months

The term "Recursive Language Model" was coined by **Alex L. Zhang, Tim Kraska, and Omar Khattab** at MIT CSAIL's OASYS Lab. Zhang first proposed the concept in an October 2025 blog post, which was formalized as an arXiv preprint (2512.24601) on December 31, 2025, with a second version following on January 28, 2026. Omar Khattab is also the creator of DSPy, the prominent LLM programming framework at Stanford NLP — a connection that has accelerated RLM adoption. The paper introduced **RLM-Qwen3-8B**, the first model natively post-trained to operate recursively, which outperformed its base Qwen3-8B by **28.3%** on average across three long-context tasks and approached GPT-5 quality.

The response was swift. **Prime Intellect** published a blog post on January 1, 2026 calling RLMs "the paradigm of 2026" and announcing it as a major research focus. DSPy added `dspy.RLM` as a first-class module alongside `dspy.ChainOfThought` and `dspy.ReAct`. Google's Agent Development Kit received a community RLM implementation. VentureBeat covered the work under the headline "MIT's new recursive framework lets LLMs process 10 million tokens." Multiple independent reimplementations appeared on GitHub within weeks.

Several related but distinct papers share the recursive theme without using the exact "RLM" term. **PRefLexOR** (Buehler, MIT, 2024; published in *npj Artificial Intelligence* 2025) uses "recursive language modeling" to describe iterative reasoning refinement with thinking tokens — a different mechanism focused on self-refinement rather than recursive self-invocation. **ReDel** (Zhu et al., UPenn, EMNLP 2024) provides a toolkit for recursive multi-agent delegation. **RISE** (Qu et al., CMU/DeepMind, NeurIPS 2024) trains models to recursively introspect and correct across multiple turns. **ThReaD** (NAACL 2025) models LLM generation as threads that can recursively spawn child threads for sub-problems.

## How RLMs actually work: context as environment, not input

The core architectural innovation of RLMs is deceptively simple: **the prompt is not fed into the transformer's context window**. Instead, the RLM wraps a standard LLM with a thin interface that stores the input in a persistent **Python REPL** as a string variable. The LLM then writes code to inspect, search, slice, and transform this external context programmatically, spawning recursive calls to itself or other models on isolated snippets.

The mechanism operates through a two-level structure. A "root LM" (at depth 0) receives only the user's query — not the full document or corpus. The full input sits in the REPL environment as a variable. The root LM interacts with this environment through a loop: it generates code, executes it, observes results, and can call `sub_lm(prompt)` to recursively invoke fresh LM instances on subsets of the context. These sub-LMs can themselves spawn further recursive calls, creating an arbitrarily deep call tree. Results flow back up through the REPL's variable space.

This design yields several properties that standard autoregressive models lack. **Context capacity becomes effectively unbounded** — tested on inputs exceeding 10 million tokens, two orders of magnitude beyond typical context windows. **Context rot is mitigated** because each recursive call operates on a small, focused snippet rather than competing with thousands of irrelevant tokens for attention. **Computational complexity can drop to O(log N)** for sparse retrieval tasks, since the model can implement binary search through code rather than attending over the full input linearly. And critically, the combined system of LM + Python REPL + recursion is **Turing-complete**, whereas a standard transformer has bounded computational expressiveness.

The paper demonstrates a two-agent architecture: a more capable "root language model" (e.g., GPT-5) serves as the orchestrator, while a smaller "recursive language model" (e.g., GPT-5-mini) handles sub-queries. On the OOLONG benchmark, **RLM using GPT-5-mini outperformed standalone GPT-5 by more than 2×** while being cheaper per query. On BrowseComp-Plus, which requires answering questions from inputs of 6–11 million tokens, RLM-GPT-5 scored **91.33%** versus **0%** for base models that simply could not process such inputs.

## A taxonomy of recursion across the LLM landscape

The word "recursive" appears throughout AI/ML literature but refers to fundamentally different things depending on context. Understanding these distinctions is essential.

**Traditional Recursive Neural Networks (RvNNs)**, pioneered by Richard Socher and Christopher Manning at Stanford (2011–2013), apply the same weight matrices recursively over tree-structured inputs — typically syntactic parse trees — to compose word representations into phrase and sentence representations bottom-up. The recursion is baked into the **network architecture**. RvNNs were small models (millions of parameters) designed for compositional semantics tasks like sentiment analysis. They require pre-defined tree structures and use backpropagation through structure (BPTS) for training. Recurrent Neural Networks (RNNs) are technically a special case of RvNNs where the tree degenerates into a linear chain.

**Looped/recursive transformer architectures** apply the same transformer block iteratively with weight-tying across depth. The **Universal Transformer** (Dehghani et al., 2019, ICLR) introduced this idea with adaptive computation time, allowing each position to be revised a variable number of times. This line achieved Turing completeness — Giannou et al. (ICML 2023) proved that a **13-layer looped transformer** can emulate a one-instruction set computer. The **Universal Reasoning Model** (2025) extended this approach with convolutional gating, achieving state-of-the-art on ARC-AGI benchmarks. These represent architectural recursion at training and inference time, distinct from both RvNNs and RLMs.

**RLMs** operate at yet another level: the recursion is in the **inference procedure**, not the architecture. The underlying model is a standard transformer. The recursive behavior emerges from how the model is invoked — through a code-execution loop that allows the model to spawn new instances of itself. This makes RLMs a **systems-level paradigm** rather than an architectural innovation. Any sufficiently capable LLM can be wrapped as an RLM without modifying its weights (though post-training with RL improves recursive behavior).

The relationship between RLMs and structured reasoning frameworks forms a clear progression. **Chain-of-Thought (CoT)** is linear, sequential reasoning within a single context window — no recursion at all. **Tree-of-Thought (ToT)** adds branching and backtracking via external search orchestration, making it structurally recursive at the orchestration layer, but each LLM call still operates within a single context. **Graph-of-Thought (GoT)** generalizes to arbitrary graph topologies with merging and feedback loops. **Algorithm-of-Thought (AoT)** internalizes algorithmic patterns (including recursive ones) within a single prompt. **Reasoning models like OpenAI's o1/o3 and DeepSeek R1** internalize recursive strategies — decomposition, backtracking, self-refinement — through RL training, but execute them entirely within token space. RLMs uniquely **separate variable space from token space**, storing information in external memory and processing it through programmatic recursion.

| Approach | Recursion type | Where reasoning happens | Context handling |
|---|---|---|---|
| CoT | None (linear) | Single context window | Bounded by window |
| ToT | Orchestration-level tree search | Multiple LLM calls | Single context per call |
| GoT | Graph traversal with merging | Multiple LLM calls | Single context per call |
| o1/o3/R1 | Implicit, learned via RL | Internal reasoning tokens | Bounded by window |
| Traditional RvNNs | Architectural (shared weights) | Within neural network layers | Fixed tree structure |
| **RLMs** | **Explicit programmatic** | **Multiple LM calls + REPL** | **External, unbounded** |

## Where recursive patterns already work in practice

While the RLM paradigm itself is pre-production, recursive patterns in LLMs are already deployed across multiple application domains with measurable impact.

**Recursive summarization** is the most mature production application. OpenAI's 2021 work on recursively summarizing books with human feedback established the template: chunk documents, summarize each chunk, then recursively summarize the summaries. **RAPTOR** (ICLR 2024) extends this by building recursive tree-structured retrieval indices, improving accuracy on the QuALITY benchmark by **20% absolute** with GPT-4. Map-reduce summarization is now a standard pattern in LangChain, LlamaIndex, and Google Cloud Workflows for processing contracts, transcripts, and reports.

**Self-refinement loops** are widely deployed in coding assistants and content generation. The **Self-Refine** framework (Madaan et al., NeurIPS 2023) showed that a single LLM acting as generator, critic, and refiner improves outputs by approximately **20% absolute** across seven tasks. **Reflexion** (Shinn et al., 2023) implements "verbal reinforcement learning" where agents reflect on failures, store reflections in memory, and improve in subsequent attempts — completing 130 of 134 AlfWorld tasks. These patterns are built into GitHub Copilot, Cursor, and enterprise agentic frameworks. However, an important counterpoint from Huang and Chen (2023) demonstrates that without external feedback signals, LLMs struggle to genuinely self-correct reasoning errors — they may "correct" correct answers into incorrect ones.

**Recursive theorem proving** has shown remarkable recent results. Google's **HILBERT** (2025) uses recursive subgoal decomposition to achieve **99.2% on miniF2F** (6.6 points above the best public method) and **70.0% on PutnamBench** (a 422% improvement). **DeepSeek-Prover-V2**, a 671B-parameter model, achieves 88.9% on miniF2F through recursive proof search. These systems remain research tools but are approaching practical utility for working mathematicians.

**Recursive task decomposition** powers enterprise AI workflows. Least-to-Most Prompting, Decomposed Prompting (DecomP), and Divide-and-Conquer Prompting all break complex problems into recursively simpler subproblems. **LADDER** (2025) applies this to self-improvement: a model recursively generates progressively easier variants of hard problems, uses them for RL training, and improved a Llama 3B model from **2% to 82%** on undergraduate integration problems.

Across all these applications, recursive approaches consistently outperform single-pass methods by **5–40%** depending on the task, but share common limitations: **2–10× compute cost** multiplied by recursion depth, **diminishing returns** after 2–3 iterations, and **information loss** at each recursion level.

## The field in early 2026: crystallizing but unproven at scale

The RLM paradigm occupies an unusual position: it has generated extraordinary excitement within weeks of publication but remains entirely unproven in production. The paper explicitly describes itself as "proof-of-concept over production optimization," noting that sequential execution, lack of caching, and absence of cost/latency budgets limit current deployability.

Several signals suggest rapid maturation. **DSPy's native `dspy.RLM` module** positions RLMs as a first-class inference strategy alongside established paradigms. Prime Intellect's **RLMEnv** — a persistent Python REPL environment for long-horizon agents — integrates RLMs into their RL training stack, with their INTELLECT-3 model. A Google ADK implementation reimplements the original codebase in "a more enterprise-ready format." The open-source RLM repository supports Docker, Modal, and local sandboxes with a visualizer for recursive call trees.

The broader ecosystem of recursive LLM approaches is converging from multiple directions. Looped transformers are proving that architectural recursion improves reasoning (ARC-AGI state-of-the-art). Reasoning models demonstrate that RL can teach implicit recursive strategies. Self-refinement and recursive decomposition are production-proven patterns. RLMs synthesize these threads into a unified, programmable framework.

## Conclusion

"Recursive Language Model" is crystallizing into a specific, well-defined term for a paradigm where LLMs programmatically manage their own context through recursive self-invocation — a concept distinct from both classic recursive neural networks (architectural weight-sharing over trees) and reasoning models (implicit recursive patterns learned via RL). The Zhang-Kraska-Khattab framework's key insight is that **long prompts should be environment variables, not neural network inputs**, transforming context management from an architectural scaling problem into a program synthesis problem. While the term is barely two months old as a formal paper, the underlying recursive patterns — summarization, self-refinement, decomposition, and recursive planning — are already production-proven across industries. The open question is not whether recursive approaches work (they demonstrably do, improving performance by 5–40% across tasks) but whether the unified RLM paradigm will become the standard abstraction. Its integration into DSPy and adoption by research labs like Prime Intellect suggests it will — but production validation, cost optimization, and peer review remain ahead. The field should treat RLMs as a **promising, well-motivated framework backed by strong preliminary results**, not yet an established paradigm.