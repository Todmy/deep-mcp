llm:
  provider: "openai"              # openai | deepseek | glm | openrouter | local
  root_model: "o3-mini"           # reasoning model for root LM (depth 0)
  sub_model: "gpt-4o-mini"        # cheaper model for sub-LM calls (depth 1+)
  api_key: "${OPENAI_API_KEY}"    # or set RLM_API_KEY env var
  # base_url: "https://api.openai.com/v1"  # override for custom endpoints

search:
  provider: "tavily"              # tavily | brave | searxng
  api_key: "${TAVILY_API_KEY}"    # not needed for searxng
  # base_url: "http://localhost:8080"  # for self-hosted searxng

engine:
  max_recursion_depth: 3          # 1-10
  max_turns: 30                   # max code-execute-observe iterations
  max_sub_lm_calls: 50            # total sub_lm budget
  timeout_per_exec: 30            # seconds per code block execution
